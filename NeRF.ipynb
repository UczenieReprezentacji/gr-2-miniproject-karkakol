{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "691613195ed85d4e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Wstęp do NeRF\n",
    "W tym notebooku przedstawimy czym jest NeRF, do czego służy oraz jakie techniki są używane aby usprawnić jego działanie.\n",
    "\n",
    "#### Czym jest NeRF ?\n",
    "\n",
    "NeRF to akronim od Neural Radiance Fields for View Synthesis, co można przetłumaczyć jako Neuronowe Pola Radiacyjne do syntezy widoków.\n",
    "\n",
    "Jest to technika wykorzystująca głębokie sieci neuronowe do generowania realistycznych renderów scen trójwymiarowych. Głównym celem NeRF jest modelowanie radiance field, czyli funkcji reprezentującej radiancję światła w trójwymiarowej przestrzeni. Prościej mówiąc, NeRF jest w stanie wygenerować fotorealistyczne obrazy scen 3D, na podstawie danych treningowych z rzeczywistych scen. \n",
    "\n",
    "Tradycyjne metody renderowania 3D, takie jak ray tracing czy rasterization, mają swoje ograniczenia w generowaniu szczegółowych i fotorealistycznych obrazów. NeRF stanowi innowacyjne podejście, umożliwiając renderowanie scen 3D poprzez uczenie głębokich sieci neuronowych na podstawie danych treningowych z rzeczywistych scen.\n",
    "\n",
    "<!-- 3 videos in row -->\n",
    "<div style=\"display: flex; justify-content: center;\">\n",
    "<video width=\"420\" height=\"420\" controls>\n",
    "  <source src=\"data/blender_paper_lego_spiral_200000_rgb.mp4\" type=\"video/mp4\" controls loop>\n",
    "  Your browser does not support the video tag.\n",
    "</video>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f8d0e7",
   "metadata": {},
   "source": [
    "### W jaki sposób NeRF przetwarza input\n",
    "\n",
    "Reprezentujemy ciągłą scenę jako funkcję wektorową o wartościach 5D, której dane wejściowe to współrzędne w przestrzeni 3D i kierunek widzenia, a wyjściem jest kolor i gęstość objętości w danym punkcie.\n",
    "\n",
    "Aby zbliżyć się do tej ciągłej reprezentacji sceny 5D, NeRF używa sieci neuronowej o nazwie Multi-Layer Perceptron (MLP) oznaczonej poniżej jako $F_\\Theta$.\n",
    "\n",
    "----\n",
    "$$\\Large F_\\Theta: (x, d) → (c, σ) $$\n",
    "gdzie:\n",
    "\n",
    "$F_\\Theta$ - sieć neuronowa <br/>\n",
    "$x = (x, y, z)$ - współrzędne punktu w przestrzeni 3D <br/>\n",
    "$c = (r, g, b)$ - kolor emitowany w danym punkcie <br/>\n",
    "$d = (\\theta, \\phi)$ - kierunek widzenia <br/>\n",
    "$\\sigma$ - gęstość objętości\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "Proces ten jest realizowany poprzez przekształcenie danych wejściowych (w jaki sposób jest to realizowane jest w części Fourier Features) i przekazanie ich do sieci neuronowej. Sieć neuronowa zawiera 8 warstw ukrytych, każda z 256 neuronami oraz aktywacją ReLU. Sieć na początku otrzymuje przetworzony wektor współrzednych (o wielkości 60), następnie w piątej warstwie dodawany jest ponownie ten wektor (żeby sieć dalej uwzględniała współrzędne punktu). W przedostatniej warstwie sieci zwracany jest gęstość objętości, a w celu uzyskania koloru emitowanego dodawany jest jeszcze przetworzony wektor kierunku widzenia (o wielkości 40). Architektura sieci neuronowej została przedstawiona poniżej.\n",
    "\n",
    "![NeRF architecture](src/mlp.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b5355e",
   "metadata": {},
   "source": [
    "**Pytanie do publiczności**: Dlaczego wektory $x$ i $d$ są przetwarzane? Czy nie można było ich od razu przekazać do sieci neuronowej?\n",
    "\n",
    "### Czym jest input dla NeRF?\n",
    "Jako dataset otrzymujemy zbiór zdjęć z różnych perspektyw, ale sieć MLP potrzebuje danych w postaci $(x, d)$, gdzie wynikiem jest kolor i gęstość objętości w danym punkcie. Więc w jaki sposób możemy przekształcić zdjęcia z perspektywy na takie dane oraz jak trenować sieć neuronową? \n",
    "\n",
    "W celu przekształcenia zdjęć i pozycjami kamer na dane wejściowe dla sieci neuronowej, wykorzystujemy promienie (ang. rays). Promień to linia prosta łącząca kamerę z punktem na zdjęciu i przechodząca za nim. Z części promienia znajdującej się za punktem na zdjęciu, wybieramy równomiernie $n$ punktów, które są przetwarzane i przekazywane do sieci neuronowej. Otrzymujemy w ten sposób $n$ par $(x, d)$ posortowanych według odległości od kamery.\n",
    "\n",
    "![](src/mlp_io.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcab764d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58d6d602",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bdc47f801f5eab66",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## W jaki sposób NeRF renderuje obrazy?\n",
    "NeRF tworzy obraz 2D za pomocą techniki zwanej renderowaniem volumetrycznym. Ta technika tworzy obraz przez śledzenie promienia (linii) przechodzącej przez każdy piksel wirtualnej kamery, przechodzącego przez scenę. Dane jest to następującym wzorem:\n",
    "\n",
    "( można o tym wyżej wzorze myśleć jak o wartości oczekiwanej koloru biorąc pod uwage zakres długości promienia, koloru powierzchni przez które przechodzi promień oraz gestości obiektu przez który promień przechodzi )\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"padding: 20px;\">\n",
    "<img src=\"src/render.png\" width=\"1000\" />\n",
    "</div>\n",
    "<br>\n",
    "<div style=\"padding: 10px;\">\n",
    "<img src=\"src/dlugosc_toru_promienia.png\" width=\"500\" />\n",
    "</div>\n",
    "\n",
    "\n",
    "tn - dolny zakres długości promienia\n",
    "tf - górny zakres długości promienia\n",
    "r(t) - koniec wektora zaczepionego w punkcie o ( czyli pozycja obserwatora ) o zasiegu wektora t oraz kierunku wektora danym przez d\n",
    "<br>\n",
    "<div style=\"padding: 10px;\">\n",
    "<img src=\"src/promienie_gestosc.png\" width=\"1000\" />\n",
    "</div>\n",
    "\n",
    "Rysunek poglądowy może pomóc zrozumieć jak gęstośc otoczenia i odległość promienia może wpływać na kolor oraz jak ten kolor potem używać do obliczania loss'u (g.t. - ground truth ;) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da311b4187019070",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Pytanie 2\n",
    "### Opisz swoimi słowami co robią poszczególne czynniki w poniższym wzorze na kolor?\n",
    "\n",
    "<div style=\"padding: 20px;\">\n",
    "<img src=\"src/render.png\" width=\"1000\" />\n",
    "</div>\n",
    "<br>\n",
    "<div style=\"padding: 10px;\">\n",
    "<img src=\"src/dlugosc_toru_promienia.png\" width=\"500\" />\n",
    "</div>\n",
    "\n",
    "<---- Miejsce na odpowiedź ---->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dab194038ed27a5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Usprawnienie modelu\n",
    "\n",
    "#### Jedną z najwazniszych rzeczy jakie można zrobić to Fourier Feature\n",
    "\n",
    "Operowanie na suchych współrzednych x,y,z przynosi słabe rezultaty w kontekście szczegółów i kolorów na obrazku, autorzy pracy powołując sie na innych badaczy dokonują następującej transformacji:\n",
    "\n",
    "<div style=\"padding: 10px;\">\n",
    "<img src=\"src/fourier_feature.png\" width=\"2000\" />\n",
    "</div>\n",
    "\n",
    "gdzie \n",
    "$p = x * B$\n",
    "<br>\n",
    "$B -$ macierz gausowska z rozkładu `N(0, σ^2)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29915756d4a1e7fb",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Poniższy przykład pokazuje jak może wygląda nauka z użyciem Fourier Feature i bez niego, gołym okiem widać na obrazku oraz na lossie różnice\n",
    "\n",
    "<div style=\"padding: 10px;\">\n",
    "<img src=\"src/lion_none_gauss_v1.gif\" width=\"2000\" />\n",
    "</div>\n",
    "\n",
    "Należy jednak uważać, ponieważ nieodpowiednie dobranie parametrów może doprowadzić do pogorszenia jakości zdjeć, tyczy sie to parametru L\n",
    "\n",
    "<div style=\"padding: 10px;\">\n",
    "<img src=\"src/test_sweep_1e-4_5000_more_low.gif\" width=\"2000\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b562f77ad7502809",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Proces trenowania\n",
    "\n",
    "Trenowanie tej sieci można przedstawić w następujących krokach:\n",
    "\n",
    "1. Odpowienie przygotowanie danych. Bedziemy potrzebować wykonanych zdjęć wraz z pozcją kamery.\n",
    "2. W każdej iteracji jest losowany zestaw promieni dla których bedzie liczony kolor\n",
    "3. Dla każdego wylosowanego promienia jest obliczanych N wylosowanych próbek na całej jego długości\n",
    "4. Z wylosowanych próbek za pomocą renderowania objętości jest obliczany kolor\n",
    "5. Ostatnim krokiem jest obliczanie loss'u który jest dany wzorem: \n",
    "\n",
    "<div style=\"padding: 10px;\">\n",
    "<img src=\"src/loss.png\" width=\"500\" />\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908ac60b0d4acf3c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Rezultaty\n",
    "\n",
    "<div style=\"padding: 10px;\">\n",
    "<img src=\"src/res3.png\" width=\"2000\" />\n",
    "</div>\n",
    "\n",
    "<div style=\"padding: 10px;\">\n",
    "<img src=\"src/res2.png\" width=\"2000\" />\n",
    "</div>\n",
    "\n",
    "<div style=\"padding: 10px;\">\n",
    "<img src=\"src/res1.png\" width=\"2000\" />\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98f07e823a146c7",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "760a0b2f5faa0a9c",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7cc0bebb8d0adc54",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Zródła:\n",
    "\n",
    "https://www.matthewtancik.com/nerf\n",
    "https://dl.acm.org/doi/pdf/10.1145/3503250\n",
    "https://towardsdatascience.com/nerf-representing-scenes-as-neural-radiance-fields-for-view-synthesis-ef1e8cebace4\n",
    "https://bmild.github.io/fourfeat/\n",
    "https://arxiv.org/pdf/2006.10739.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9052b7e9ff4891",
   "metadata": {
    "collapsed": false
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

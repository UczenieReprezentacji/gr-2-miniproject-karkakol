{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "691613195ed85d4e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Wstęp do NeRF\n",
    "W tym notebooku przedstawimy czym jest NeRF, do czego służy oraz jakie techniki są używane aby usprawnić jego działanie.\n",
    "\n",
    "#### Czym jest NeRF ?\n",
    "\n",
    "NeRF to akronim od Neural Radiance Fields for View Synthesis, co można przetłumaczyć jako Neuronowe Pola Radiacyjne do syntezy widoków.\n",
    "\n",
    "Jest to technika wykorzystująca głębokie sieci neuronowe do generowania realistycznych renderów scen trójwymiarowych. Głównym celem NeRF jest modelowanie radiance field, czyli funkcji reprezentującej radiancję światła w trójwymiarowej przestrzeni. Prościej mówiąc, NeRF jest w stanie wygenerować fotorealistyczne obrazy scen 3D, na podstawie danych treningowych z rzeczywistych scen. \n",
    "\n",
    "Tradycyjne metody renderowania 3D, takie jak ray tracing czy rasterization, mają swoje ograniczenia w generowaniu szczegółowych i fotorealistycznych obrazów. NeRF stanowi innowacyjne podejście, umożliwiając renderowanie scen 3D poprzez uczenie głębokich sieci neuronowych na podstawie danych treningowych z rzeczywistych scen.\n",
    "\n",
    "\n",
    "<div style=\"padding: 40px;\">\n",
    "<img src=\"src/f1.png\" width=\"2000\" />\n",
    "</div>\n",
    "\n",
    "\n",
    "## W jaki sposób NeRF przetwarza input\n",
    "\n",
    "Reprezentujemy ciągłą scenę jako funkcję wektorową o wartościach 5D, której dane wejściowe to współrzędne (x,y,z) i 2D kierunek widzenia (θ, φ), a wyjście to emitowany kolor \n",
    "\n",
    "Przybliżamy tę ciągłą reprezentację sceny 5D za pomocą sieci :\n",
    "<br>\n",
    "### $ MLP - FΘ: (x, d) → (c, σ) $\n",
    "\n",
    "#### $x = $ (x, y, z) \n",
    "\n",
    "#### $d -$ kierunek    wektora     w   przestrzeni  trójwymiarowej\n",
    "\n",
    "#### $c =$ (r, g, b) \n",
    "\n",
    "#### $σ -$ gęstośc punktów / gęstośc objętości ( jak dużo w danym punkcie znajduje sie materii )\n",
    "\n",
    "\n",
    "i optymalizujemy jej wagi Θ, aby mapować z każdej wejściowej współrzędnej 5D do odpowiedniej gęstości objętości i emitowanego kierunkowego koloru. Aby to osiągnąć, MLP FΘ najpierw przetwarza dane wejściowe 3D koordynatu x za pomocą 8 w pełni połączonych warstw (używając aktywacji ReLU i 256 kanałów na warstwę), a następnie wyjścia σ i 256-wymiarowego wektora cech. Ten wektor cech następnie jest łączony z kierunkiem widzenia promienia kamery i przekazywany do dodatkowej, w pełni połączonej warstwy (używając aktywacji ReLU i 128 kanałów), która wyjściowo emituje zależny od widzenia kolor RGB.\n",
    "\n",
    "<div style=\"padding: 40px;\">\n",
    "<img src=\"src/nerf_layers.png\" width=\"2000\" />\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d73c82bba0b1a54",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Pytanie 1\n",
    "### Dlaczego dodawany jest wektor w przestrzeni 3D ( oznaczenie jako d ) jako input do tej sieci?\n",
    "\n",
    "<---- Miejsce na odpowiedź ---->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc47f801f5eab66",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## W jaki sposób NeRF renderuje obrazy?\n",
    "NeRF tworzy obraz 2D za pomocą techniki zwanej renderowaniem volumetrycznym. Ta technika tworzy obraz przez śledzenie promienia (linii) przechodzącej przez każdy piksel wirtualnej kamery, przechodzącego przez scenę. Dane jest to następującym wzorem:\n",
    "\n",
    "( można o tym wyżej wzorze myśleć jak o wartości oczekiwanej koloru biorąc pod uwage zakres długości promienia, koloru powierzchni przez które przechodzi promień oraz gestości obiektu przez który promień przechodzi )\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"padding: 20px;\">\n",
    "<img src=\"src/render.png\" width=\"1000\" />\n",
    "</div>\n",
    "<br>\n",
    "<div style=\"padding: 10px;\">\n",
    "<img src=\"src/dlugosc_toru_promienia.png\" width=\"500\" />\n",
    "</div>\n",
    "\n",
    "\n",
    "tn - dolny zakres długości promienia\n",
    "tf - górny zakres długości promienia\n",
    "r(t) - koniec wektora zaczepionego w punkcie o ( czyli pozycja obserwatora ) o zasiegu wektora t oraz kierunku wektora danym przez d\n",
    "<br>\n",
    "<div style=\"padding: 10px;\">\n",
    "<img src=\"src/promienie_gestosc.png\" width=\"1000\" />\n",
    "</div>\n",
    "\n",
    "Rysunek poglądowy może pomóc zrozumieć jak gęstośc otoczenia i odległość promienia może wpływać na kolor oraz jak ten kolor potem używać do obliczania loss'u (g.t. - ground truth ;) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da311b4187019070",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Pytanie 2\n",
    "### Opisz swoimi słowami co robią poszczególne czynniki w poniższym wzorze na kolor?\n",
    "\n",
    "<div style=\"padding: 20px;\">\n",
    "<img src=\"src/render.png\" width=\"1000\" />\n",
    "</div>\n",
    "<br>\n",
    "<div style=\"padding: 10px;\">\n",
    "<img src=\"src/dlugosc_toru_promienia.png\" width=\"500\" />\n",
    "</div>\n",
    "\n",
    "<---- Miejsce na odpowiedź ---->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dab194038ed27a5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Usprawnienie modelu\n",
    "\n",
    "#### Jedną z najwazniszych rzeczy jakie można zrobić to Fourier Feature\n",
    "\n",
    "Operowanie na suchych współrzednych x,y,z przynosi słabe rezultaty w kontekście szczegółów i kolorów na obrazku, autorzy pracy powołując sie na innych badaczy dokonują następującej transformacji:\n",
    "\n",
    "<div style=\"padding: 10px;\">\n",
    "<img src=\"src/fourier_feature.png\" width=\"2000\" />\n",
    "</div>\n",
    "\n",
    "gdzie \n",
    "$p = x * B$\n",
    "<br>\n",
    "$B -$ macierz gausowska z rozkładu `N(0, σ^2)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29915756d4a1e7fb",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Poniższy przykład pokazuje jak może wygląda nauka z użyciem Fourier Feature i bez niego, gołym okiem widać na obrazku oraz na lossie różnice\n",
    "\n",
    "<div style=\"padding: 10px;\">\n",
    "<img src=\"src/lion_none_gauss_v1.gif\" width=\"2000\" />\n",
    "</div>\n",
    "\n",
    "Należy jednak uważać, ponieważ nieodpowiednie dobranie parametrów może doprowadzić do pogorszenia jakości zdjeć, tyczy sie to parametru L\n",
    "\n",
    "<div style=\"padding: 10px;\">\n",
    "<img src=\"src/test_sweep_1e-4_5000_more_low.gif\" width=\"2000\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b562f77ad7502809",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Proces trenowania\n",
    "\n",
    "Trenowanie tej sieci można przedstawić w następujących krokach:\n",
    "\n",
    "1. Odpowienie przygotowanie danych. Bedziemy potrzebować wykonanych zdjęć wraz z pozcją kamery.\n",
    "2. W każdej iteracji jest losowany zestaw promieni dla których bedzie liczony kolor\n",
    "3. Dla każdego wylosowanego promienia jest obliczanych N wylosowanych próbek na całej jego długości\n",
    "4. Z wylosowanych próbek za pomocą renderowania objętości jest obliczany kolor\n",
    "5. Ostatnim krokiem jest obliczanie loss'u który jest dany wzorem: \n",
    "\n",
    "<div style=\"padding: 10px;\">\n",
    "<img src=\"src/loss.png\" width=\"500\" />\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908ac60b0d4acf3c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Rezultaty\n",
    "\n",
    "<div style=\"padding: 10px;\">\n",
    "<img src=\"src/res3.png\" width=\"2000\" />\n",
    "</div>\n",
    "\n",
    "<div style=\"padding: 10px;\">\n",
    "<img src=\"src/res2.png\" width=\"2000\" />\n",
    "</div>\n",
    "\n",
    "<div style=\"padding: 10px;\">\n",
    "<img src=\"src/res1.png\" width=\"2000\" />\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98f07e823a146c7",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "760a0b2f5faa0a9c",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7cc0bebb8d0adc54",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Zródła:\n",
    "\n",
    "https://www.matthewtancik.com/nerf\n",
    "https://dl.acm.org/doi/pdf/10.1145/3503250\n",
    "https://towardsdatascience.com/nerf-representing-scenes-as-neural-radiance-fields-for-view-synthesis-ef1e8cebace4\n",
    "https://bmild.github.io/fourfeat/\n",
    "https://arxiv.org/pdf/2006.10739.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9052b7e9ff4891",
   "metadata": {
    "collapsed": false
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
